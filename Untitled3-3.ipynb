{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb27755d",
   "metadata": {},
   "source": [
    "# Next Word Prediction Model using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a860c",
   "metadata": {},
   "source": [
    "Importing the necessary Python libraries and the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6d04446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Read the text file\n",
    "with open('/Users/mrunalipatil/Downloads/sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d53a52d",
   "metadata": {},
   "source": [
    "Let’s tokenize the text to create a sequence of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e6de05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de3ca5d",
   "metadata": {},
   "source": [
    "The text is divided into individual words or tokens through tokenization. A Tokenizer object is created to perform this task. The fit_on_texts method builds a vocabulary of unique words from the text, each assigned a numerical index. The total_words variable then counts these unique words, indicating the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de9db06",
   "metadata": {},
   "source": [
    "Splitting the text into sequences of tokens and forming n-grams from the sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de808e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f6a9b",
   "metadata": {},
   "source": [
    "The text is split into lines using '\\n' as a delimiter. Each line is converted to a sequence of numerical tokens using the texts_to_sequences method. A for loop generates n-grams (subsequences) from these tokens, with each n-gram's last token as the target word. These n-grams are collected in the input_sequences list for training a word prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a40bcd",
   "metadata": {},
   "source": [
    "Let’s pad the input sequences to have equal length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "342e763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ec694",
   "metadata": {},
   "source": [
    "The pad_sequences function adjusts all input sequences to the same length, determined by max_sequence_len, which is the length of the longest sequence. It pads shorter sequences at the beginning (padding=pre) to match this maximum length. The resulting sequences are then converted into a numpy array for uniformity and ease of processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa1f45",
   "metadata": {},
   "source": [
    "Let’s split the sequences into input and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11d1bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60404461",
   "metadata": {},
   "source": [
    "Converting the output array into a suitable format for training a model, where each target word is represented as a binary vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95bc3129",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb02a9",
   "metadata": {},
   "source": [
    "Building a neural network architecture to train the model:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aae9c96e",
   "metadata": {},
   "source": [
    "Model Foundation: A Sequential model is initialized, serving as a linear stack for neural network layers.\n",
    "\n",
    "Embedding Layer: This first layer, the Embedding layer, transforms input sequences into fixed-size dense vectors.\n",
    "\n",
    "Embedding Layer Parameters: It's configured with 'total_words' for vocabulary size, '100' for the size of word embeddings, and 'input_length' for the length of input sequences.\n",
    "\n",
    "LSTM Layer Addition: Following is an LSTM layer, a recurrent neural network component, with 150 units to capture sequential data patterns.\n",
    "\n",
    "LSTM Layer Function: The LSTM layer learns 150 distinct internal representations or memory cells.\n",
    "\n",
    "Dense Layer Integration: A Dense layer is added as the final layer to generate output predictions.\n",
    "\n",
    "Dense Layer Characteristics: It comprises 'total_words' units and employs a softmax activation function to convert output scores into probabilistic predictions of the next word in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3965d3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 17, 100)           820000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 150)               150600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8200)              1238200   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2208800 (8.43 MB)\n",
      "Trainable params: 2208800 (8.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c26fe1",
   "metadata": {},
   "source": [
    "Compile and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62176883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3010/3010 [==============================] - 42s 14ms/step - loss: 6.2209 - accuracy: 0.0777\n",
      "Epoch 2/100\n",
      "3010/3010 [==============================] - 43s 14ms/step - loss: 5.5023 - accuracy: 0.1248\n",
      "Epoch 3/100\n",
      "3010/3010 [==============================] - 43s 14ms/step - loss: 5.1083 - accuracy: 0.1476\n",
      "Epoch 4/100\n",
      "3010/3010 [==============================] - 43s 14ms/step - loss: 4.7712 - accuracy: 0.1670\n",
      "Epoch 5/100\n",
      "3010/3010 [==============================] - 44s 14ms/step - loss: 4.4634 - accuracy: 0.1849\n",
      "Epoch 6/100\n",
      "3010/3010 [==============================] - 44s 15ms/step - loss: 4.1779 - accuracy: 0.2051\n",
      "Epoch 7/100\n",
      "3010/3010 [==============================] - 44s 14ms/step - loss: 3.9053 - accuracy: 0.2320\n",
      "Epoch 8/100\n",
      "3010/3010 [==============================] - 43s 14ms/step - loss: 3.6489 - accuracy: 0.2620\n",
      "Epoch 9/100\n",
      "3010/3010 [==============================] - 43s 14ms/step - loss: 3.4045 - accuracy: 0.2961\n",
      "Epoch 10/100\n",
      "3010/3010 [==============================] - 43s 14ms/step - loss: 3.1788 - accuracy: 0.3305\n",
      "Epoch 11/100\n",
      "3010/3010 [==============================] - 43s 14ms/step - loss: 2.9705 - accuracy: 0.3650\n",
      "Epoch 12/100\n",
      "3010/3010 [==============================] - 42s 14ms/step - loss: 2.7781 - accuracy: 0.4002\n",
      "Epoch 13/100\n",
      "3010/3010 [==============================] - 42s 14ms/step - loss: 2.6009 - accuracy: 0.4319\n",
      "Epoch 14/100\n",
      "3010/3010 [==============================] - 42s 14ms/step - loss: 2.4384 - accuracy: 0.4645\n",
      "Epoch 15/100\n",
      "3010/3010 [==============================] - 43s 14ms/step - loss: 2.2890 - accuracy: 0.4934\n",
      "Epoch 16/100\n",
      "3010/3010 [==============================] - 44s 15ms/step - loss: 2.1528 - accuracy: 0.5214\n",
      "Epoch 17/100\n",
      "3010/3010 [==============================] - 43s 14ms/step - loss: 2.0264 - accuracy: 0.5475\n",
      "Epoch 18/100\n",
      "3010/3010 [==============================] - 42s 14ms/step - loss: 1.9109 - accuracy: 0.5728\n",
      "Epoch 19/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 1.8048 - accuracy: 0.5948\n",
      "Epoch 20/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 1.7062 - accuracy: 0.6167\n",
      "Epoch 21/100\n",
      "3010/3010 [==============================] - 42s 14ms/step - loss: 1.6172 - accuracy: 0.6364\n",
      "Epoch 22/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 1.5332 - accuracy: 0.6548\n",
      "Epoch 23/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 1.4592 - accuracy: 0.6707\n",
      "Epoch 24/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 1.3877 - accuracy: 0.6859\n",
      "Epoch 25/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 1.3240 - accuracy: 0.7012\n",
      "Epoch 26/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 1.2650 - accuracy: 0.7142\n",
      "Epoch 27/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 1.2121 - accuracy: 0.7257\n",
      "Epoch 28/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 1.1597 - accuracy: 0.7360\n",
      "Epoch 29/100\n",
      "3010/3010 [==============================] - 39s 13ms/step - loss: 1.1164 - accuracy: 0.7457\n",
      "Epoch 30/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 1.0715 - accuracy: 0.7570\n",
      "Epoch 31/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 1.0340 - accuracy: 0.7638\n",
      "Epoch 32/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 0.9950 - accuracy: 0.7732\n",
      "Epoch 33/100\n",
      "3010/3010 [==============================] - 38s 13ms/step - loss: 0.9643 - accuracy: 0.7784\n",
      "Epoch 34/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.9320 - accuracy: 0.7857\n",
      "Epoch 35/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.9045 - accuracy: 0.7930\n",
      "Epoch 36/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 0.8761 - accuracy: 0.7977\n",
      "Epoch 37/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.8516 - accuracy: 0.8034\n",
      "Epoch 38/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.8289 - accuracy: 0.8084\n",
      "Epoch 39/100\n",
      "3010/3010 [==============================] - 39s 13ms/step - loss: 0.8098 - accuracy: 0.8125\n",
      "Epoch 40/100\n",
      "3010/3010 [==============================] - 39s 13ms/step - loss: 0.7893 - accuracy: 0.8169\n",
      "Epoch 41/100\n",
      "3010/3010 [==============================] - 39s 13ms/step - loss: 0.7733 - accuracy: 0.8192\n",
      "Epoch 42/100\n",
      "3010/3010 [==============================] - 39s 13ms/step - loss: 0.7555 - accuracy: 0.8238\n",
      "Epoch 43/100\n",
      "3010/3010 [==============================] - 39s 13ms/step - loss: 0.7413 - accuracy: 0.8269\n",
      "Epoch 44/100\n",
      "3010/3010 [==============================] - 39s 13ms/step - loss: 0.7298 - accuracy: 0.8273\n",
      "Epoch 45/100\n",
      "3010/3010 [==============================] - 39s 13ms/step - loss: 0.7123 - accuracy: 0.8324\n",
      "Epoch 46/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.7007 - accuracy: 0.8341\n",
      "Epoch 47/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.6892 - accuracy: 0.8360\n",
      "Epoch 48/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.6770 - accuracy: 0.8389\n",
      "Epoch 49/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 0.6721 - accuracy: 0.8392\n",
      "Epoch 50/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.6583 - accuracy: 0.8428\n",
      "Epoch 51/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 0.6480 - accuracy: 0.8446\n",
      "Epoch 52/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 0.6469 - accuracy: 0.8429\n",
      "Epoch 53/100\n",
      "3010/3010 [==============================] - 39s 13ms/step - loss: 0.6357 - accuracy: 0.8471\n",
      "Epoch 54/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 0.6273 - accuracy: 0.8486\n",
      "Epoch 55/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 0.6251 - accuracy: 0.8481\n",
      "Epoch 56/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.6190 - accuracy: 0.8485\n",
      "Epoch 57/100\n",
      "3010/3010 [==============================] - 39s 13ms/step - loss: 0.6045 - accuracy: 0.8534\n",
      "Epoch 58/100\n",
      "3010/3010 [==============================] - 39s 13ms/step - loss: 0.6051 - accuracy: 0.8513\n",
      "Epoch 59/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.5993 - accuracy: 0.8525\n",
      "Epoch 60/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 0.5876 - accuracy: 0.8550\n",
      "Epoch 61/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.5868 - accuracy: 0.8554\n",
      "Epoch 62/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.5858 - accuracy: 0.8535\n",
      "Epoch 63/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 0.5821 - accuracy: 0.8547\n",
      "Epoch 64/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.5797 - accuracy: 0.8556\n",
      "Epoch 65/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.5740 - accuracy: 0.8566\n",
      "Epoch 66/100\n",
      "3010/3010 [==============================] - 44s 15ms/step - loss: 0.5749 - accuracy: 0.8550\n",
      "Epoch 67/100\n",
      "3010/3010 [==============================] - 57s 19ms/step - loss: 0.5650 - accuracy: 0.8591\n",
      "Epoch 68/100\n",
      "3010/3010 [==============================] - 50s 17ms/step - loss: 0.5705 - accuracy: 0.8548\n",
      "Epoch 69/100\n",
      "3010/3010 [==============================] - 51s 17ms/step - loss: 0.5628 - accuracy: 0.8583\n",
      "Epoch 70/100\n",
      "3010/3010 [==============================] - 52s 17ms/step - loss: 0.5630 - accuracy: 0.8574\n",
      "Epoch 71/100\n",
      "3010/3010 [==============================] - 52s 17ms/step - loss: 0.5592 - accuracy: 0.8585\n",
      "Epoch 72/100\n",
      "3010/3010 [==============================] - 51s 17ms/step - loss: 0.5577 - accuracy: 0.8584\n",
      "Epoch 73/100\n",
      "3010/3010 [==============================] - 50s 17ms/step - loss: 0.5524 - accuracy: 0.8601\n",
      "Epoch 74/100\n",
      "3010/3010 [==============================] - 49s 16ms/step - loss: 0.5510 - accuracy: 0.8605\n",
      "Epoch 75/100\n",
      "3010/3010 [==============================] - 52s 17ms/step - loss: 0.5499 - accuracy: 0.8607\n",
      "Epoch 76/100\n",
      "3010/3010 [==============================] - 53s 18ms/step - loss: 0.5446 - accuracy: 0.8616\n",
      "Epoch 77/100\n",
      "3010/3010 [==============================] - 53s 17ms/step - loss: 0.5438 - accuracy: 0.8613\n",
      "Epoch 78/100\n",
      "3010/3010 [==============================] - 92s 31ms/step - loss: 0.5432 - accuracy: 0.8604\n",
      "Epoch 79/100\n",
      "3010/3010 [==============================] - 51s 17ms/step - loss: 0.5428 - accuracy: 0.8604\n",
      "Epoch 80/100\n",
      "3010/3010 [==============================] - 50s 17ms/step - loss: 0.5412 - accuracy: 0.8607\n",
      "Epoch 81/100\n",
      "3010/3010 [==============================] - 54s 18ms/step - loss: 0.5412 - accuracy: 0.8618\n",
      "Epoch 82/100\n",
      "3010/3010 [==============================] - 52s 17ms/step - loss: 0.5385 - accuracy: 0.8612\n",
      "Epoch 83/100\n",
      "3010/3010 [==============================] - 79s 26ms/step - loss: 0.5373 - accuracy: 0.8614\n",
      "Epoch 84/100\n",
      "3010/3010 [==============================] - 56s 19ms/step - loss: 0.5378 - accuracy: 0.8599\n",
      "Epoch 85/100\n",
      "3010/3010 [==============================] - 59s 20ms/step - loss: 0.5326 - accuracy: 0.8632\n",
      "Epoch 86/100\n",
      "3010/3010 [==============================] - 58s 19ms/step - loss: 0.5291 - accuracy: 0.8632\n",
      "Epoch 87/100\n",
      "3010/3010 [==============================] - 55s 18ms/step - loss: 0.5329 - accuracy: 0.8609\n",
      "Epoch 88/100\n",
      "3010/3010 [==============================] - 56s 19ms/step - loss: 0.5312 - accuracy: 0.8607\n",
      "Epoch 89/100\n",
      "3010/3010 [==============================] - 53s 18ms/step - loss: 0.5331 - accuracy: 0.8619\n",
      "Epoch 90/100\n",
      "3010/3010 [==============================] - 42s 14ms/step - loss: 0.5267 - accuracy: 0.8628\n",
      "Epoch 91/100\n",
      "3010/3010 [==============================] - 44s 14ms/step - loss: 0.5306 - accuracy: 0.8616\n",
      "Epoch 92/100\n",
      "3010/3010 [==============================] - 43s 14ms/step - loss: 0.5233 - accuracy: 0.8633\n",
      "Epoch 93/100\n",
      "3010/3010 [==============================] - 43s 14ms/step - loss: 0.5212 - accuracy: 0.8639\n",
      "Epoch 94/100\n",
      "3010/3010 [==============================] - 42s 14ms/step - loss: 0.5293 - accuracy: 0.8606\n",
      "Epoch 95/100\n",
      "3010/3010 [==============================] - 41s 14ms/step - loss: 0.5245 - accuracy: 0.8620\n",
      "Epoch 96/100\n",
      "3010/3010 [==============================] - 44s 15ms/step - loss: 0.5276 - accuracy: 0.8620\n",
      "Epoch 97/100\n",
      "3010/3010 [==============================] - 43s 14ms/step - loss: 0.5275 - accuracy: 0.8620\n",
      "Epoch 98/100\n",
      "3010/3010 [==============================] - 42s 14ms/step - loss: 0.5238 - accuracy: 0.8622\n",
      "Epoch 99/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.5243 - accuracy: 0.8621\n",
      "Epoch 100/100\n",
      "3010/3010 [==============================] - 40s 13ms/step - loss: 0.5187 - accuracy: 0.8639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17cb17410>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fbb380",
   "metadata": {},
   "source": [
    "The ‘metrics’ parameter is set to ‘accuracy’ to monitor the accuracy during training. After compiling the model, the ‘fit’ method is called to train the model on the input sequences ‘X’ and the corresponding output ‘y’. The ‘epochs’ parameter specifies the number of times the training process will iterate over the entire dataset. The ‘verbose’ parameter is set to ‘1’ to display the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c296c3a3",
   "metadata": {},
   "source": [
    "Now Let's generate the next word predictions using our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bf8c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 331ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "I will leave if they may have been\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"I will leave if they\"\n",
    "next_words = 3\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cb4f0cd",
   "metadata": {},
   "source": [
    "Initialization: The process starts with seed_text as initial text and next_words specifying the number of words to predict.\n",
    "\n",
    "Tokenization and Padding: In each iteration of the loop, seed_text is tokenized and then padded to match the maximum sequence length.\n",
    "\n",
    "Word Prediction: The model's predict method is used on the padded token sequence to predict the next word.\n",
    "Selecting Predicted Word: The word with the highest probability is chosen as the prediction using np.argmax.\n",
    "\n",
    "Text Extension: The predicted word is added to seed_text, and this process repeats for the number of times specified in next_words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
